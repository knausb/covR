---
title: "Introduction to read_matrix"
author: "Brian J. Knaus"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to read_matrix}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

`read_matrix` is a package designed for fast input of large gzipped datasets.


## Preliminaries


A major bottleneck in bioinformatics projects is reading large quantities of data from the disk to memory and writing it back again.
The tools provided by R were not really intended for the large file sizes encountered in bioinformatics.
For example, `read.table` can be frustratingly slow.
There are a few alternatives existing.
The fastest option is probably [data.table::fread](http://cran.r-project.org/package=data.table).
It's one shortcoming is that it doesn't currently read in compressed files.
In Unix environments this can be addressed with `zcat` or `gzcat` inside the fread call.
Another option is [readr](http://cran.r-project.org/package=readr).


I've also decided to make my own attempt at making a fast read function with the hope it may provide flexibility not available in the above examples.
One of the challenges of reading data into memory is that we frequently don't know how much data is contained in a file.
This is problematic because growing a data structure, as each line is read in for example, usuually results in a performance cost.
If you know how much data you're going to read in you can initiallize a data structure for this size.
Based on this I've adopted a philosophy where we'll take an initial pass through the file to figure out how much data is in it.
Once we have this information, or perhaps you already had it, we can read in the data.


## Workflow

I've included an small example file with the package.
You can find it as such.


```{r}
library(coveRage)
ex_file <- system.file("extdata", "sc10_4k.mpileup.gz", package = "coveRage")
```


We can now take our first pass through the file to collect information.

```{r}
stats <- file_stats(ex_file, verbose=0)
stats
```


The number of columns and rows in the file will allow us to initialize the number of columns and rows we'll need in our matrix.

```{r}
x1 <- read_matrix(ex_file, nrows=stats['Rows'], cols=1:stats['Columns'], verbose=0)
x1[1:6, 1:9]
```



This is mpileup format data.
The first three columns identify the chromosome, position and reference allele.
Samples occur in subsequent columns where each samples consists of three columns (here I've included the optional depth column).
The second sample here, columns 7, 8 and 9, has zero depth and values are not reported.


We can use the column specification to read in subsets of the file.
Let's say we want to read in samples and and three and omit the depth information.



```{r}
x2 <- read_matrix(ex_file, nrows=stats['Rows'], cols=c(1:3, 5:6, 11:12), verbose=0)
head(x2)
```


Note that the format of the data is a matrix of characters.


```{r}
class(x2)
class(x2[1,1])
```


Henry Ford reportedly said that customers could have their cars in any color they wanted, as long as it was black.
That's sort of the philosophy I've taken here with the hope that this will improve performance.



## Future directions



The difference in execution time between `file_stats` and `read_matrix` is due to manipulating the data in memory.
This suggests some improvements may be made in the code.
One path I'd like to pursue will be to make `read_matrix` parallelized to improve performance.
I'll have to learn a few new tricks before I accomplish that though.


